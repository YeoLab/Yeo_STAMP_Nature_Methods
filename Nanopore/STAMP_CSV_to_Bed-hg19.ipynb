{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pybedtools import BedTool\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import binom\n",
    "from scipy.special import betainc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total(df):\n",
    "    if df['DP']<1:\n",
    "        return  0\n",
    "    else:\n",
    "        return (1- betainc(df['total_miss'], df['total_match'], 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir= '/Path to save directory/'\n",
    "output_dir= '/Path to save directory/tmp_bedgraphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-01f7db2ff2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*hg19.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dir' is not defined"
     ]
    }
   ],
   "source": [
    "beds = sorted(glob.glob(os.path.join(input_dir, '*Filename.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports csv files, C>T and G>A filters and assigns confidence score\n",
    "for bed in beds:\n",
    "    df=pd.read_csv(bed)\n",
    "    tmp1=df[df['3']=='C']\n",
    "    tmp1=tmp1[tmp1['miss_base']=='T']\n",
    "    tmp2=df[df['3']=='G']\n",
    "    tmp2=tmp2[tmp2['miss_base']=='A']\n",
    "    df=pd.concat([tmp1,tmp2])\n",
    "    df['stop']=df['1']+1\n",
    "    df['total_match']=df['fwd']+df['rev']\n",
    "    df['total_miss']=df['fwd_miss']+df['rev_miss']\n",
    "    df['total_fraction']=df['total_miss']/(df['total_miss']+df['total_match'])\n",
    "    df['total_conf']=df.apply(total, axis=1)\n",
    "    df=df[df['total_conf']>=0.5]\n",
    "    df[['0','1','stop','total_fraction','total_conf','3']].to_csv(bed+'.conf0.5_TA_filtered.bed', sep='\\t', header=None, index=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = sorted(glob.glob(os.path.join(output_dir, '*filtered.bed')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/oasis/tscc/scratch/d2lorenz/nanopore/promethion/mpileup/tmp_bedgraphs/APOBEC_control_df_hg19.csv.conf0.5_TA_filtered.bed',\n",
       " '/oasis/tscc/scratch/d2lorenz/nanopore/promethion/mpileup/tmp_bedgraphs/RBFOX_df_hg19.csv.conf0.5_TA_filtered.bed',\n",
       " '/oasis/tscc/scratch/d2lorenz/nanopore/promethion/mpileup/tmp_bedgraphs/RPS2_df_hg19.csv.conf0.5_TA_filtered.bed',\n",
       " '/oasis/tscc/scratch/d2lorenz/nanopore/promethion/mpileup/tmp_bedgraphs/SRSF_df_hg19.csv.conf0.5_TA_filtered.bed']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Intersects sites to get strand info based on gene\n",
    "gtf=BedTool('/projects/ps-yeolab3/bay001/annotations/hg19/gencode_v19/gencode.v19.annotation.gtf')\n",
    "for bed in beds:\n",
    "    df=BedTool(bed)\n",
    "    df1=df.intersect(gtf, wa=True,wb=True)\n",
    "    SP=output_dir+bed.split('/')[9]+'.annotated.bed'\n",
    "    df1.saveas(SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=0\n",
    "df1=0\n",
    "SP=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = sorted(glob.glob(os.path.join(output_dir, '*.annotated.bed')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stranded_f(df):\n",
    "    if (df[12]=='+')&(df[5]=='C'):\n",
    "        return  1\n",
    "    elif (df[12]=='-')&(df[5]=='G'):\n",
    "        return  1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666182\n",
      "1565278\n",
      "856785\n",
      "1526687\n",
      "1438227\n",
      "862585\n",
      "1467828\n",
      "1381112\n",
      "853284\n",
      "1327692\n",
      "1241108\n",
      "717628\n"
     ]
    }
   ],
   "source": [
    "##Assign strands based on gene and applies GC filter. Removes ambiguous sites \n",
    "for bed in beds:\n",
    "    df=pd.read_csv(bed, sep='\\t', header=None)\n",
    "    df=df[df[8]=='exon']\n",
    "    print(len(df.drop_duplicates([0,1,12])))\n",
    "    df=df.drop_duplicates([0,1,12]).drop_duplicates([0,1],keep=False)\n",
    "    print(len(df))\n",
    "    df['filter']=df.apply(stranded_f, axis=1)\n",
    "    df=df[df['filter']==1]\n",
    "    print(len(df))\n",
    "    SP=bed+'.exon_strand.bed'\n",
    "    df[[0,1,2,3,4,12]].to_csv(SP, sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = sorted(glob.glob(os.path.join(output_dir, '*.exon_strand.bed')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(beds[0], sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "##Makes bedgraphs with no filters\n",
    "for bed in beds:\n",
    "    tmp=pd.read_csv(bed, sep='\\t', header=None)\n",
    "    SP=bed+'.0.5sailor.fraction.none.bedgraph'\n",
    "    tmp[[0,1,2,3,4,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    SP=bed+'.0.5sailor.sailor_conf.none.bedgraph'\n",
    "    tmp[[0,1,2,4,3,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    tmp=tmp[tmp[4]>=0.99]\n",
    "    SP=bed+'.0.99sailor.fraction.none.bedgraph'\n",
    "    tmp[[0,1,2,3,4,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    SP=bed+'.0.99sailor.sailor_conf.none.bedgraph'\n",
    "    tmp[[0,1,2,4,3,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "##makes bedgraphs filtering for unique sites (across all CSVs)\n",
    "for bed in beds:\n",
    "    tmp=pd.read_csv(bed, sep='\\t', header=None)\n",
    "    tmp['gene']=bed.split(\"/\")[9].split('_')[0]\n",
    "    df=pd.concat([df,tmp])\n",
    "df1=df[df[4]>=0.5].drop_duplicates([0,1,5], keep=False).copy()\n",
    "for bed in beds:\n",
    "    gene=bed.split(\"/\")[9].split('_')[0]\n",
    "    tmp=df1[df1['gene']==gene].copy()\n",
    "    SP=bed+'.0.5sailor.fraction.all.bedgraph'\n",
    "    tmp[[0,1,2,3,4,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    SP=bed+'.0.5sailor.sailor_conf.all.bedgraph'\n",
    "    tmp[[0,1,2,4,3,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "df1=df[df[4]>=0.99].drop_duplicates([0,1,5], keep=False).copy()\n",
    "for bed in beds:\n",
    "    gene=bed.split(\"/\")[9].split('_')[0]\n",
    "    tmp=df1[df1['gene']==gene].copy()\n",
    "    SP=bed+'.0.99sailor.fraction.all.bedgraph'\n",
    "    tmp[[0,1,2,3,4,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    SP=bed+'.0.99sailor.sailor_conf.all.bedgraph'\n",
    "    tmp[[0,1,2,4,3,5]].to_csv(SP, sep='\\t', header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Makes bedgraphs filtering for APO sites\n",
    "for bed in beds:\n",
    "    gene=bed.split(\"/\")[9].split('_')[0]\n",
    "    tmp=df[df['gene'].isin(['APOBEC',gene])].copy()\n",
    "    tmp2=tmp[tmp[4]>=0.5].drop_duplicates([0,1,5], keep=False)\n",
    "    tmp2=tmp2[tmp2['gene']==gene].copy()\n",
    "    SP=bed+'.0.5sailor.fraction.APO.bedgraph'\n",
    "    tmp2[[0,1,2,3,4,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    SP=bed+'.0.5sailor.sailor_conf.APO.bedgraph'\n",
    "    tmp2[[0,1,2,4,3,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    tmp2=tmp[tmp[4]>=0.99].drop_duplicates([0,1,5], keep=False)\n",
    "    tmp2=tmp2[tmp2['gene']==gene].copy()\n",
    "    SP=bed+'.0.99sailor.fraction.APO.bedgraph'\n",
    "    tmp2[[0,1,2,3,4,5]].to_csv(SP, sep='\\t', header=None, index=None)\n",
    "    SP=bed+'.0.99sailor.sailor_conf.APO.bedgraph'\n",
    "    tmp2[[0,1,2,4,3,5]].to_csv(SP, sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir='/Path to save directory/tmp_bedgraphs/0_based/'\n",
    "beds = sorted(glob.glob(os.path.join(output_dir, '*.bedgraph')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Makes bedgraphs 0 instead of 1 based\n",
    "for bed in beds:\n",
    "    df= pd.read_csv(bed, sep='\\t', header=None)\n",
    "    gene=bed.split(\"/\")[9].split('_')[0]\n",
    "    FN=bed.split(\"/\")[9].split('bed.')[3]\n",
    "    df[1]=df[1]-1\n",
    "    df[2]=df[2]-1\n",
    "    SP=final_dir+gene+'_0-based.'+FN\n",
    "    df.to_csv(SP, sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "beds = sorted(glob.glob(os.path.join(final_dir, '*.bedgraph')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Removes SNPs\n",
    "SNPS=BedTool('/projects/ps-yeolab3/bay001/annotations/hg19/hg19.commonSNPs147.bed3')\n",
    "for bed in beds:\n",
    "    df=BedTool(bed)\n",
    "    df1=df.subtract(SNPS)\n",
    "    SP=final_dir+bed.split(\"/\")[10].split('bedgraph')[0]+'rmSNP.bedgraph'\n",
    "    df1.saveas(SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848624\n",
      "19277560\n",
      "848624\n"
     ]
    }
   ],
   "source": [
    "##site_bed files (EditC)\n",
    "input_dir= '/Path to save directory/'\n",
    "output_dir= '/Path to save directory/cDNA_hg19/'\n",
    "beds2_dir='/Path to save directory//d-cDNA/'\n",
    "beds = sorted(glob.glob(os.path.join(input_dir, '*hg19.csv')))\n",
    "for bed in beds:\n",
    "    df= pd.read_csv(bed)\n",
    "    data=bed.split('/')[8].split('_')[0]\n",
    "    df2=pd.read_csv(beds2_dir+str(data)+'_0-based.0.5sailor.fraction.APO.rmSNP.bedgraph', header=None, sep='\\t',names=[0,1,2,3,4,'strand'])\n",
    "    df['key']=df['0']+\":\"+df['1'].astype(str)\n",
    "    df2['key']=df2[0]+\":\"+df2[2].astype(str)\n",
    "    set1=set(df['key'])\n",
    "    set2=set(df2['key'])\n",
    "    print(len(set2))\n",
    "    len(set1.intersection(set2))\n",
    "    mapping = dict(df2[['key', 'strand']].values)\n",
    "    df['strand_real'] = df.key.map(mapping)\n",
    "    print(len(df))\n",
    "    print(len(df.dropna()))\n",
    "    df['total_match']=df['fwd']+df['rev']\n",
    "    df['total_miss']=df['fwd_miss']+df['rev_miss']\n",
    "    df['total']=df['total_match']+df['total_miss']\n",
    "    df['miss,total']=df['total_miss'].astype(str)+','+df['total'].astype(str)\n",
    "    df['total_confidence']=df.apply(total, axis=1)\n",
    "    SP=output_dir+data+\"_cDNA_hg19_sites.bed\"\n",
    "    df['stop']=df['1']+1\n",
    "    df=df.fillna(0)\n",
    "    df[['0','1','stop','total_confidence','miss,total','strand_real']].to_csv(SP, sep='\\t', header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3essential (tscc)",
   "language": "python",
   "name": "python3essential"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
